{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "outputId": "e9222d7f-cd64-428f-a2d0-76217c2e553f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p /content/csc421/a3/\n",
        "%cd /content/csc421/a3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Collecting Pillow==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.6MB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mtorchvision 0.2.2.post3 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.49 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 4.1.1\n",
            "    Uninstalling Pillow-4.1.1:\n",
            "      Successfully uninstalled Pillow-4.1.1\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "cell_type": "markdown",
      "source": [
        "# Helper code"
      ]
    },
    {
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ]
    },
    {
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "import math\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "cell_type": "markdown",
      "source": [
        "## Data loader"
      ]
    },
    {
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "cell_type": "code",
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "metadata": {
        "id": "wa5-onJhoSeM"
      },
      "cell_type": "code",
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "      \n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "      fig.colorbar(cax)\n",
        "\n",
        "      # Set up axes\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "      # Show label at every tick\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      # Add title\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "      plt.tight_layout()\n",
        "      plt.grid('off')\n",
        "      plt.show()\n",
        "      #plt.savefig(save)\n",
        "\n",
        "      #plt.close(fig)\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "              \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                         hidden_size=opts.hidden_size, \n",
        "                         opts=opts)\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXNsLNkOn38w"
      },
      "cell_type": "markdown",
      "source": [
        "# Your code for NMT models"
      ]
    },
    {
      "metadata": {
        "id": "_BAfi_8yWB3y"
      },
      "cell_type": "markdown",
      "source": [
        "## GRU cell"
      ]
    },
    {
      "metadata": {
        "id": "9ztmyA5Ro67o"
      },
      "cell_type": "code",
      "source": [
        "from torch.nn.parameter import Parameter\n",
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        ## Input linear layers\n",
        "        self.Wiz = Parameter(torch.Tensor(hidden_size, input_size))\n",
        "        self.Wir = Parameter(torch.Tensor(hidden_size, input_size))\n",
        "        self.Win = Parameter(torch.Tensor(hidden_size, input_size))\n",
        "\n",
        "        ## Hidden linear layers\n",
        "        self.Whz = Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.Whr = Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.Whn = Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        \n",
        "        self.br = Parameter(torch.Tensor(hidden_size,1))\n",
        "        self.bz = Parameter(torch.Tensor(hidden_size,1))\n",
        "        self.bg = Parameter(torch.Tensor(hidden_size,1))\n",
        "        \n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        \n",
        "        x = x.transpose(1,0)\n",
        "        h_prev = h_prev.transpose(1,0)\n",
        "        \n",
        "        z = F.sigmoid(torch.mm(self.Wiz, x) + torch.mm(self.Whz, h_prev) + self.br)\n",
        "        r = F.sigmoid(torch.mm(self.Wir, x) + torch.mm(self.Whz, h_prev) + self.bz)\n",
        "        g = F.tanh(torch.mm(self.Win, x) + r * (torch.mm(self.Whn, h_prev) + self.bg))\n",
        "        h_new = (1-z)*g + z*h_prev\n",
        "        return h_new.transpose(1,0)\n",
        "        \n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JBVFLEZWNC1"
      },
      "cell_type": "markdown",
      "source": [
        "### GRU encoder / decoder"
      ]
    },
    {
      "metadata": {
        "id": "xaDt7XDmWRzC"
      },
      "cell_type": "code",
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)\n",
        "\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        #self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tWe0RO5FWajD"
      },
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ]
    },
    {
      "metadata": {
        "id": "9GUK5A7CWhV8"
      },
      "cell_type": "code",
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size, seq_len, hidden_size = keys.size()\n",
        "        expanded_queries = queries.unsqueeze(1).expand_as(keys)\n",
        "        concat_inputs = torch.cat((expanded_queries, keys), 2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        \n",
        "        context = torch.bmm(attention_weights.view(batch_size, 1, seq_len), values)\n",
        "        return context, attention_weights\n",
        "        \n",
        "      \n",
        "\n",
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k = seq_len_decoder)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len, hidden_size = keys.size()\n",
        "        \n",
        "        if (len(queries.size()) == 2):\n",
        "          queries = queries.unsqueeze(1)\n",
        "         \n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        \n",
        "        unnormalized_attention = torch.bmm(k, torch.transpose(q, 1, 2))\n",
        "        unnormalized_attention = unnormalized_attention * self.scaling_factor\n",
        "        \n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        \n",
        "        context = torch.bmm(attention_weights.transpose(1,2), v)\n",
        "        \n",
        "        return context, attention_weights\n",
        "      \n",
        "      \n",
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1/k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len, hidden_size = keys.size()\n",
        "        \n",
        "        if (len(queries.size()) == 2):\n",
        "          queries = queries.unsqueeze(1)\n",
        "        \n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        \n",
        "        unnormalized_attention = torch.bmm(self.K(k), torch.transpose(q, 1, 2))\n",
        "        unnormalized_attention = unnormalized_attention * self.scaling_factor\n",
        "        \n",
        "        mask = torch.tril(self.neg_inf*torch.ones(unnormalized_attention.shape)).cuda()\n",
        "        \n",
        "        attention_weights = self.softmax(unnormalized_attention + mask)\n",
        "        \n",
        "        context = torch.bmm(attention_weights.transpose(1,2), v)\n",
        "        \n",
        "        return context, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pemjZo2XWtRt"
      },
      "cell_type": "markdown",
      "source": [
        "### Attention decoder"
      ]
    },
    {
      "metadata": {
        "id": "PfjF0Z-PWwPv"
      },
      "cell_type": "code",
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        #self.rnn = nn.GRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init # batch_size x hidden_size\n",
        "        for i in range(seq_len):\n",
        "            \n",
        "            embed_current = embed[:,i,:]\n",
        "            context, attention_weights = self.attention.forward(h_prev, annotations, annotations)\n",
        "            embed_and_context = torch.cat((embed_current, torch.squeeze(context,dim=1) ), dim=1)\n",
        "            \n",
        "            h_prev = self.rnn(embed_and_context, h_prev)\n",
        "\n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N8JpcwTRW5cw"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformer decoder"
      ]
    },
    {
      "metadata": {
        "id": "V5vJPku1W7sz"
      },
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        #self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "        #                            hidden_size=hidden_size, \n",
        "        #                         ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x k (= 1 or decoder_seq_len))\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x dec_seq_len x hidden_size        \n",
        "        _,enc_seq,hid = annotations.size()\n",
        "        \n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "          \n",
        "          new_contexts, self_attention_weights = self.self_attentions[i].forward(contexts, contexts, contexts) #Is annotations, annotations correct? No! I've updated this (different from what I submitted)\n",
        "          \n",
        "          residual_contexts = new_contexts + contexts\n",
        "          \n",
        "          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations)\n",
        "          \n",
        "          residual_contexts = new_contexts + residual_contexts\n",
        "          new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "          contexts = new_contexts + residual_contexts\n",
        "\n",
        "          \n",
        "          encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "          self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XuNFd6LNo0-o"
      },
      "cell_type": "markdown",
      "source": [
        "# Training\n"
      ]
    },
    {
      "metadata": {
        "id": "kiUwiOITHTW4"
      },
      "cell_type": "markdown",
      "source": [
        "## Download dataset"
      ]
    },
    {
      "metadata": {
        "id": "xwcFjsEpHRbI",
        "outputId": "b1615442-312a-4265-c48b-dd23e2f70a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_data.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_data.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hmQmyJDSRFKR"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN decoder"
      ]
    },
    {
      "metadata": {
        "id": "0LKaRF1jwhH7",
        "outputId": "6680486c-d7b2-4c03-e41b-f4b8c78e17ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2307
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('generosity', 'enerositygay')\n",
            "('cessation', 'essationcay')\n",
            "('recurrence', 'ecurrenceray')\n",
            "('valueless', 'aluelessvay')\n",
            "('paint', 'aintpay')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNNDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train loss: 2.332 | Val loss: 2.067 | Gen: ay-ay ay-ay ontay intay ontay\n",
            "Epoch:   1 | Train loss: 1.939 | Val loss: 1.918 | Gen: enway ay-ay onssay intedway onsway\n",
            "Epoch:   2 | Train loss: 1.790 | Val loss: 1.826 | Gen: entay away onseray-intedway intingway onsway\n",
            "Epoch:   3 | Train loss: 1.687 | Val loss: 1.770 | Gen: entway away onsay-ingway intingway onsway\n",
            "Epoch:   4 | Train loss: 1.615 | Val loss: 1.736 | Gen: entway away-ay-ay-ay-ay-ay- onteray ingway onsway\n",
            "Epoch:   5 | Train loss: 1.557 | Val loss: 1.712 | Gen: entedway away-ayday onsingway inway onsway\n",
            "Epoch:   6 | Train loss: 1.513 | Val loss: 1.673 | Gen: eay-ayday away oningway inway ontingway\n",
            "Epoch:   7 | Train loss: 1.466 | Val loss: 1.637 | Gen: edway away oningway isway ontingway\n",
            "Epoch:   8 | Train loss: 1.423 | Val loss: 1.619 | Gen: edway away oningnay-intershay issway ontingway\n",
            "Epoch:   9 | Train loss: 1.383 | Val loss: 1.600 | Gen: edway away-ayday oningnay issway oningway\n",
            "Epoch:  10 | Train loss: 1.354 | Val loss: 1.614 | Gen: edway away oningenay-inway-awla issway onsingeray-inway-ayd\n",
            "Epoch:  11 | Train loss: 1.338 | Val loss: 1.587 | Gen: edway away oningnay-inway-ayday issway oningway\n",
            "Epoch:  12 | Train loss: 1.316 | Val loss: 1.540 | Gen: eday away oningnay issway onsway-inway-ayday\n",
            "Epoch:  13 | Train loss: 1.288 | Val loss: 1.535 | Gen: edway away-awlay oningnay issway oningnay\n",
            "Epoch:  14 | Train loss: 1.269 | Val loss: 1.566 | Gen: edway away-ayday oningnay-ayday issway onsway-ayday\n",
            "Epoch:  15 | Train loss: 1.248 | Val loss: 1.512 | Gen: eday arsway oningnay-inway-ayday issway onsway-ortitionway\n",
            "Epoch:  16 | Train loss: 1.224 | Val loss: 1.545 | Gen: eday arsay oningnay-awlay issway onsway\n",
            "Epoch:  17 | Train loss: 1.214 | Val loss: 1.522 | Gen: edway away-awlay oniningnay issway onstray-inway-ayday\n",
            "Epoch:  18 | Train loss: 1.203 | Val loss: 1.538 | Gen: edway arshedway oniningnay issway ondway-ortionway\n",
            "Epoch:  19 | Train loss: 1.187 | Val loss: 1.566 | Gen: edway arshay ondingnay-ortionway- issway onstentway\n",
            "Epoch:  20 | Train loss: 1.182 | Val loss: 1.534 | Gen: eway arsway oningnay isway onsthay-ortay-oday\n",
            "Epoch:  21 | Train loss: 1.155 | Val loss: 1.480 | Gen: eedway arshay ondingnay issway onsthentway\n",
            "Epoch:  22 | Train loss: 1.143 | Val loss: 1.503 | Gen: eway array ondingnay isway onsthedway\n",
            "Epoch:  23 | Train loss: 1.135 | Val loss: 1.547 | Gen: eway arsway ondingnay isway ondentway\n",
            "Epoch:  24 | Train loss: 1.130 | Val loss: 1.547 | Gen: efay ardway ondingnay istway ondencedway\n",
            "Epoch:  25 | Train loss: 1.112 | Val loss: 1.578 | Gen: eway away-ayday ondingway isway ondentway\n",
            "Epoch:  26 | Train loss: 1.100 | Val loss: 1.492 | Gen: eway arway ondingnay isway ondensway\n",
            "Epoch:  27 | Train loss: 1.091 | Val loss: 1.513 | Gen: eway array oningnay isway ondoundway\n",
            "Epoch:  28 | Train loss: 1.078 | Val loss: 1.522 | Gen: eway arityway ondingnay isway ondensway\n",
            "Epoch:  29 | Train loss: 1.067 | Val loss: 1.544 | Gen: eway arryway ondingnay isway ondomay-ortionway\n",
            "Epoch:  30 | Train loss: 1.058 | Val loss: 1.537 | Gen: eway arityway ondingnay-ortay-inwa isway ondnedway\n",
            "Epoch:  31 | Train loss: 1.055 | Val loss: 1.534 | Gen: eway arway ondingnay isway ondomnay\n",
            "Epoch:  32 | Train loss: 1.046 | Val loss: 1.513 | Gen: eway arityway ondingnay isway ondnedway\n",
            "Epoch:  33 | Train loss: 1.032 | Val loss: 1.499 | Gen: eway arityway ondingnay isway ondomstay-orway-ayda\n",
            "Epoch:  34 | Train loss: 1.024 | Val loss: 1.517 | Gen: eway arityway ondingnay-orway-ayda isway ondomnationway\n",
            "Epoch:  35 | Train loss: 1.026 | Val loss: 1.496 | Gen: efay arityway ondnngnay isway ondomstay-awlay\n",
            "Epoch:  36 | Train loss: 1.013 | Val loss: 1.536 | Gen: efay arityway ondingnay-awlay isway ondomnationway\n",
            "Epoch:  37 | Train loss: 1.011 | Val loss: 1.475 | Gen: efay arityway ondingnay isway ondounsway\n",
            "Epoch:  38 | Train loss: 0.996 | Val loss: 1.473 | Gen: efay arityway ondingnay isway ondoundway\n",
            "Epoch:  39 | Train loss: 0.989 | Val loss: 1.514 | Gen: efay arityway ondingnay isway ondnomstay\n",
            "Epoch:  40 | Train loss: 0.988 | Val loss: 1.538 | Gen: ehway arityway ondingnay-owngay-ood isway ondounsway-oodway\n",
            "Epoch:  41 | Train loss: 0.983 | Val loss: 1.482 | Gen: efay arityway ondingnay isway ondoundway\n",
            "Epoch:  42 | Train loss: 0.981 | Val loss: 1.449 | Gen: efay arityway ondingnentway isway ondnomstay\n",
            "Epoch:  43 | Train loss: 0.964 | Val loss: 1.465 | Gen: efyway aypray ondingnay isway ondounsway\n",
            "Epoch:  44 | Train loss: 1.001 | Val loss: 1.529 | Gen: efway ay-atunedway onnvingnessway isway ondnormentway\n",
            "Epoch:  45 | Train loss: 0.994 | Val loss: 1.466 | Gen: efay arityway ondningnay isway ondounsway\n",
            "Epoch:  46 | Train loss: 0.963 | Val loss: 1.484 | Gen: efay aypray ondnngingway isway ondomnay\n",
            "Epoch:  47 | Train loss: 0.941 | Val loss: 1.432 | Gen: efay arityway ondningnay isway ondounsway\n",
            "Epoch:  48 | Train loss: 0.932 | Val loss: 1.468 | Gen: efay aypray ondingnay isway ondnomsray\n",
            "Epoch:  49 | Train loss: 0.935 | Val loss: 1.398 | Gen: efay aypray ondingnay isway ondousionway\n",
            "Epoch:  50 | Train loss: 0.919 | Val loss: 1.373 | Gen: efay aridway ondingnay isway ondousionsway\n",
            "Epoch:  51 | Train loss: 0.911 | Val loss: 1.395 | Gen: efay aypray ondingnay isway ondounsway\n",
            "Epoch:  52 | Train loss: 0.910 | Val loss: 1.488 | Gen: efay aypray ondingnay-oodway isway ondnomstay\n",
            "Epoch:  53 | Train loss: 0.926 | Val loss: 1.418 | Gen: efay aypray ondingnay isway odingsway\n",
            "Epoch:  54 | Train loss: 0.924 | Val loss: 1.415 | Gen: efay aypray ondningway isway ondousionsway\n",
            "Epoch:  55 | Train loss: 0.905 | Val loss: 1.384 | Gen: efway aypray ondingnay isway ondounsway\n",
            "Epoch:  56 | Train loss: 0.894 | Val loss: 1.424 | Gen: efay aypray ondningnay isway odingsway\n",
            "Epoch:  57 | Train loss: 0.896 | Val loss: 1.439 | Gen: efway aypray ondingnay-oodway isway ondnousedway\n",
            "Epoch:  58 | Train loss: 0.889 | Val loss: 1.390 | Gen: efay aypray ondingnay isway ondoustionway\n",
            "Epoch:  59 | Train loss: 0.878 | Val loss: 1.382 | Gen: efay aypray ondingnay isway ondousionway\n",
            "Epoch:  60 | Train loss: 0.875 | Val loss: 1.380 | Gen: efay aypredway ondingnay isway odingssray\n",
            "Epoch:  61 | Train loss: 0.872 | Val loss: 1.368 | Gen: efyway aypray ondingningssay isway odnormway\n",
            "Epoch:  62 | Train loss: 0.875 | Val loss: 1.354 | Gen: efay ayprhay ondingnay isway odingssray\n",
            "Epoch:  63 | Train loss: 0.873 | Val loss: 1.376 | Gen: efhay ayssay ondingningsway isway ondousionway\n",
            "Epoch:  64 | Train loss: 0.862 | Val loss: 1.379 | Gen: efyway aypryway ondingingsway isway odingssray\n",
            "Epoch:  65 | Train loss: 0.854 | Val loss: 1.347 | Gen: efway aypryway ondingingssay isway odingssray\n",
            "Epoch:  66 | Train loss: 0.845 | Val loss: 1.384 | Gen: efyway aypryway ondingnay isway ondousionway\n",
            "Epoch:  67 | Train loss: 0.846 | Val loss: 1.343 | Gen: efyway aypryway ondingingsway isway ondoustway\n",
            "Epoch:  68 | Train loss: 0.837 | Val loss: 1.402 | Gen: efyway aypryway ondingnay isway ondousionway\n",
            "Epoch:  69 | Train loss: 0.840 | Val loss: 1.344 | Gen: efhay aypryway ondingingsway isway odingssray\n",
            "Epoch:  70 | Train loss: 0.833 | Val loss: 1.376 | Gen: ehway aypryway ondingnay isway ondousionway\n",
            "Epoch:  71 | Train loss: 0.842 | Val loss: 1.340 | Gen: efhay aypryway ondingingsway isway ondoustway\n",
            "Epoch:  72 | Train loss: 0.829 | Val loss: 1.361 | Gen: ehway aystay ondingingsway isway ondousionway\n",
            "Epoch:  73 | Train loss: 0.830 | Val loss: 1.347 | Gen: efyway airway ondingingstay ishay ondoustway\n",
            "Epoch:  74 | Train loss: 0.825 | Val loss: 1.335 | Gen: efyway aystay ondingingsway isway odunionessway\n",
            "Epoch:  75 | Train loss: 0.818 | Val loss: 1.343 | Gen: efyway aypryway ondingingsway isway ondoustway\n",
            "Epoch:  76 | Train loss: 0.817 | Val loss: 1.347 | Gen: efyway aypryway ondingnay ishay ondousionway\n",
            "Epoch:  77 | Train loss: 0.822 | Val loss: 1.333 | Gen: ehway aystay ondingingsway isway odnoustway\n",
            "Epoch:  78 | Train loss: 0.822 | Val loss: 1.322 | Gen: efyway aystay ondiningway ishay odusionsway\n",
            "Epoch:  79 | Train loss: 0.820 | Val loss: 1.338 | Gen: ehway aypryway ondingningsay isway ondnorray\n",
            "Epoch:  80 | Train loss: 0.804 | Val loss: 1.330 | Gen: efhay aypryway ondingingsway isway ondoustway\n",
            "Epoch:  81 | Train loss: 0.798 | Val loss: 1.320 | Gen: efedway aystay ondingingssay ishay ondourtway\n",
            "Epoch:  82 | Train loss: 0.795 | Val loss: 1.309 | Gen: ehway aystay ondiningsway isway odustionway\n",
            "Epoch:  83 | Train loss: 0.791 | Val loss: 1.320 | Gen: efhay aystay ondingingssay ishay ondourtway\n",
            "Epoch:  84 | Train loss: 0.793 | Val loss: 1.320 | Gen: ehway aypryway ondingingsway isway odustingsway\n",
            "Epoch:  85 | Train loss: 0.792 | Val loss: 1.341 | Gen: efyway aystay ondingingsshay ishay ondustlay\n",
            "Epoch:  86 | Train loss: 0.790 | Val loss: 1.333 | Gen: ehway aystay ondiningway isway odustingsway\n",
            "Epoch:  87 | Train loss: 0.779 | Val loss: 1.310 | Gen: ehedway aystay ondingingssay ishay ondustlay\n",
            "Epoch:  88 | Train loss: 0.792 | Val loss: 1.308 | Gen: ehway aystway ondininglyway ishay odustionway\n",
            "Epoch:  89 | Train loss: 0.796 | Val loss: 1.337 | Gen: efyway aypry ondininglyway isway ondustionsay\n",
            "Epoch:  90 | Train loss: 0.783 | Val loss: 1.322 | Gen: ehedway aystway ondingingssway isway ondustlyway\n",
            "Epoch:  91 | Train loss: 0.779 | Val loss: 1.299 | Gen: efhay aystay ondingingssay ishay ondoursway\n",
            "Epoch:  92 | Train loss: 0.779 | Val loss: 1.301 | Gen: ehedway aystay ondiningstay isway odustionsay\n",
            "Epoch:  93 | Train loss: 0.772 | Val loss: 1.274 | Gen: ehway aystay ondingingsay isway ordnowray\n",
            "Epoch:  94 | Train loss: 0.769 | Val loss: 1.261 | Gen: ehway aystay ondingionsway ishay ondoustway\n",
            "Epoch:  95 | Train loss: 0.770 | Val loss: 1.305 | Gen: ehway aystay ondingingsay isway ordonay\n",
            "Epoch:  96 | Train loss: 0.774 | Val loss: 1.305 | Gen: efyway aystay ondingingssway isway oduronsway\n",
            "Epoch:  97 | Train loss: 0.773 | Val loss: 1.290 | Gen: efyway aystay ondiningionsay ishay oduronsway\n",
            "Epoch:  98 | Train loss: 0.750 | Val loss: 1.284 | Gen: efay aystay ondininglyway isway odurionshay\n",
            "Epoch:  99 | Train loss: 0.745 | Val loss: 1.276 | Gen: ehway aystay ondingingssway ishay ondurolway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehway aystay ondingingssway ishay ondurolway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p2kPGj5DFv7a",
        "outputId": "1709dd1b-244f-4e12-fd71-e387055fadc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "#TEST_SENTENCE = 'the quick brown fox jumps over the lazy dog'\n",
        "#TEST_SENTENCE = 'jumps'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehway aystay ondingingssway ishay ondurolway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7cP7nl5NRJbu"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN attention decoder"
      ]
    },
    {
      "metadata": {
        "id": "nKlyfbuPDXDR",
        "outputId": "18b01b9d-d844-4ec5-a845-4635975adbe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2445
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('generosity', 'enerositygay')\n",
            "('cessation', 'essationcay')\n",
            "('recurrence', 'ecurrenceray')\n",
            "('valueless', 'aluelessvay')\n",
            "('paint', 'aintpay')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNNAttentionDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MyGRUCell. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type AdditiveAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train loss: 2.241 | Val loss: 1.995 | Gen: inway inway ay-ay-ay-ay-ay-ay-ay insway insay\n",
            "Epoch:   1 | Train loss: 1.853 | Val loss: 1.846 | Gen: inway-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay inway-ay-ay-ay-ay-ay answay ingway\n",
            "Epoch:   2 | Train loss: 1.680 | Val loss: 1.843 | Gen: othay-ay-ay-ay-ay-ay ay-ay-ay-ay ontinnay-antinnay-an atsway insway\n",
            "Epoch:   3 | Train loss: 1.542 | Val loss: 1.641 | Gen: ottay-inday ay-ay-ay-ay-ay-ay-ay ontingway isway ingtay\n",
            "Epoch:   4 | Train loss: 1.384 | Val loss: 1.487 | Gen: ethay atilway ontiontingway isway ontay-ingway\n",
            "Epoch:   5 | Train loss: 1.205 | Val loss: 1.348 | Gen: etay-ay-ay-ay-ay-ay- arway ontiontionnay issay ortingnay\n",
            "Epoch:   6 | Train loss: 1.052 | Val loss: 1.313 | Gen: edtay airay-atray ontiontiontingway istay ortingway\n",
            "Epoch:   7 | Train loss: 0.917 | Val loss: 1.494 | Gen: eray-ay-ay-ay-ay-ay- airway onininincincincay-in istay ortingway\n",
            "Epoch:   8 | Train loss: 0.869 | Val loss: 1.101 | Gen: eatthay airway ondintingingway isway orkessway\n",
            "Epoch:   9 | Train loss: 0.720 | Val loss: 1.049 | Gen: awthttctray airway ondioniongay isway orongsway\n",
            "Epoch:  10 | Train loss: 0.618 | Val loss: 0.966 | Gen: eway airway onditingingway istay erkingway\n",
            "Epoch:  11 | Train loss: 0.578 | Val loss: 0.951 | Gen: eway airway ondimitioninginningn isway ormingsway\n",
            "Epoch:  12 | Train loss: 0.506 | Val loss: 0.765 | Gen: eway airway ondsioningway isway orkgnsway\n",
            "Epoch:  13 | Train loss: 0.418 | Val loss: 0.698 | Gen: eway array onditinnangway isway oringway\n",
            "Epoch:  14 | Train loss: 0.383 | Val loss: 0.721 | Gen: eray- airway onditinnway issway orkingway\n",
            "Epoch:  15 | Train loss: 0.356 | Val loss: 1.042 | Gen: eray- airway ondiongnay isway orkgway\n",
            "Epoch:  16 | Train loss: 0.348 | Val loss: 0.735 | Gen: eway abrway onditiongway isway orkngway\n",
            "Epoch:  17 | Train loss: 0.274 | Val loss: 0.771 | Gen: eway abray onditiongay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.276 | Val loss: 0.814 | Gen: eray airway onditiongnay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.262 | Val loss: 0.561 | Gen: eway abray onditiongnay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.230 | Val loss: 0.673 | Gen: eway abray onditioncingsay isway orkwnway\n",
            "Epoch:  21 | Train loss: 0.213 | Val loss: 0.543 | Gen: eway airway onditioncingway isway orkingway\n",
            "Epoch:  22 | Train loss: 0.249 | Val loss: 0.565 | Gen: ewhay airway ondititiongnay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.190 | Val loss: 0.497 | Gen: ethay abray onditiongnay isway oringway\n",
            "Epoch:  24 | Train loss: 0.150 | Val loss: 0.394 | Gen: etay airway onditioncingway isway orkingway\n",
            "Epoch:  25 | Train loss: 0.116 | Val loss: 0.311 | Gen: ethay airway onditioncingcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.114 | Val loss: 0.350 | Gen: etthay airway onditioncmay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.097 | Val loss: 0.346 | Gen: eththay airway onditiongingcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.116 | Val loss: 0.647 | Gen: ethay airway onditinncay isway orkwingway\n",
            "Epoch:  29 | Train loss: 0.212 | Val loss: 0.380 | Gen: etthay airway onditioncingcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.117 | Val loss: 0.398 | Gen: ethay airway onditiongingsay isway orkisgway\n",
            "Epoch:  31 | Train loss: 0.106 | Val loss: 0.275 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.210 | Val loss: 0.745 | Gen: etthay abray onditioniningnay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.207 | Val loss: 0.462 | Gen: ethay airway onditinningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.117 | Val loss: 0.296 | Gen: ethay airway onditioncingcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.072 | Val loss: 0.251 | Gen: ethay airway onditioncingcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.061 | Val loss: 0.226 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.048 | Val loss: 0.193 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.041 | Val loss: 0.193 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.033 | Val loss: 0.187 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.029 | Val loss: 0.180 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.025 | Val loss: 0.178 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.023 | Val loss: 0.177 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.021 | Val loss: 0.184 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.020 | Val loss: 0.256 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.146 | Val loss: 1.252 | Gen: ethay arway ondditinninncway istway orkingnayway\n",
            "Epoch:  46 | Train loss: 0.324 | Val loss: 0.389 | Gen: ethay airway onditingingscay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.138 | Val loss: 0.269 | Gen: ethay airway onditioncingcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.074 | Val loss: 0.555 | Gen: ethay airway onditiongcay isway orkwinway\n",
            "Epoch:  49 | Train loss: 0.076 | Val loss: 0.277 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.046 | Val loss: 0.283 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.035 | Val loss: 0.171 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.025 | Val loss: 0.145 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.019 | Val loss: 0.137 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.016 | Val loss: 0.141 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.014 | Val loss: 0.136 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.012 | Val loss: 0.137 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.011 | Val loss: 0.137 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.010 | Val loss: 0.135 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.009 | Val loss: 0.134 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.009 | Val loss: 0.133 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.008 | Val loss: 0.131 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.007 | Val loss: 0.129 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.007 | Val loss: 0.129 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.006 | Val loss: 0.128 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.006 | Val loss: 0.130 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.005 | Val loss: 0.126 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.005 | Val loss: 0.124 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.182 | Val loss: 0.933 | Gen: eththhay airway onditiningway isway orkwingway\n",
            "Epoch:  69 | Train loss: 0.148 | Val loss: 0.419 | Gen: eway airway onditioncay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.068 | Val loss: 0.235 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  71 | Train loss: 0.037 | Val loss: 0.206 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.022 | Val loss: 0.143 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.013 | Val loss: 0.140 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.012 | Val loss: 0.126 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.009 | Val loss: 0.112 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.008 | Val loss: 0.113 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.007 | Val loss: 0.112 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.006 | Val loss: 0.112 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.005 | Val loss: 0.113 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.005 | Val loss: 0.114 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.005 | Val loss: 0.114 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.004 | Val loss: 0.115 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.004 | Val loss: 0.114 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.004 | Val loss: 0.117 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.004 | Val loss: 0.119 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.003 | Val loss: 0.121 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.003 | Val loss: 0.124 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.003 | Val loss: 0.126 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.003 | Val loss: 0.127 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.003 | Val loss: 0.129 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.002 | Val loss: 0.129 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.002 | Val loss: 0.129 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.002 | Val loss: 0.132 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.002 | Val loss: 0.132 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.032 | Val loss: 0.742 | Gen: etay airway ondititioningnay isway orkinway\n",
            "Epoch:  96 | Train loss: 0.116 | Val loss: 0.307 | Gen: ethay airway onditioncingcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.047 | Val loss: 0.469 | Gen: ethay airway onditioncaningcay isway orkwaway\n",
            "Epoch:  98 | Train loss: 0.049 | Val loss: 0.206 | Gen: ethay airway onditioncingcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.018 | Val loss: 0.195 | Gen: ethay airway onditioningcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vE-hKCxhF3iR",
        "outputId": "7d8ff5fd-e8c1-44a2-94f8-cb32f13e0832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the quick brown fox jumps over the lazy dog'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe quick brown fox jumps over the lazy dog \n",
            "translated:\tethay ickquay ownbray oxfay umpsjay overway ethay azylay ogday\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eFfyJ0V00Wh7",
        "outputId": "83b95451-99b2-4961-8bbc-716f645a41ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2445
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'scaled_dot',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "s_rnn_attn_encoder, s_rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, s_rnn_attn_encoder, s_rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: scaled_dot                             \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('eight', 'eightway')\n",
            "('owe', 'oweway')\n",
            "('ungracious', 'ungraciousway')\n",
            "('compared', 'omparedcay')\n",
            "('us', 'usway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNNAttentionDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MyGRUCell. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type ScaledDotAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train loss: 2.299 | Val loss: 2.091 | Gen: ay ay onsay onsay onsay\n",
            "Epoch:   1 | Train loss: 1.818 | Val loss: 1.847 | Gen: estay ay-ingay onglay illay ongay\n",
            "Epoch:   2 | Train loss: 1.589 | Val loss: 1.736 | Gen: eway array ongtingway illay oungray\n",
            "Epoch:   3 | Train loss: 1.427 | Val loss: 1.672 | Gen: eway ay-ingay ongtiongway istay ooungray\n",
            "Epoch:   4 | Train loss: 1.283 | Val loss: 1.595 | Gen: eway ay-ay-ay-ay-ay ongiongway isay ougringway\n",
            "Epoch:   5 | Train loss: 1.171 | Val loss: 1.540 | Gen: eway ay-ay ongtiongway isay orgringway\n",
            "Epoch:   6 | Train loss: 1.053 | Val loss: 1.698 | Gen: eeway airray ongulioungiongway isay oukrigkningway\n",
            "Epoch:   7 | Train loss: 0.932 | Val loss: 1.315 | Gen: eeway ay-irway onditoungway isway orkingway\n",
            "Epoch:   8 | Train loss: 0.836 | Val loss: 1.406 | Gen: eway ay-irway ondgay-otray isway orkingway\n",
            "Epoch:   9 | Train loss: 0.798 | Val loss: 1.356 | Gen: ecay airway oniongway ishay orknay\n",
            "Epoch:  10 | Train loss: 0.703 | Val loss: 1.062 | Gen: eway ay-ayday onditiongay isay orkingway\n",
            "Epoch:  11 | Train loss: 0.596 | Val loss: 1.089 | Gen: eway airway ondgationgay isay orknongway\n",
            "Epoch:  12 | Train loss: 0.564 | Val loss: 1.049 | Gen: eway airway ondititinglay isway orkingway\n",
            "Epoch:  13 | Train loss: 0.513 | Val loss: 1.255 | Gen: eay airway ondiongway isway orknay\n",
            "Epoch:  14 | Train loss: 0.492 | Val loss: 1.012 | Gen: eway airway onditingnay isway orknhay\n",
            "Epoch:  15 | Train loss: 0.478 | Val loss: 1.014 | Gen: eway airway onditioninglay isway orknhay\n",
            "Epoch:  16 | Train loss: 0.472 | Val loss: 0.851 | Gen: ebay airway ondgtiounigcay isway orkningway\n",
            "Epoch:  17 | Train loss: 0.404 | Val loss: 0.858 | Gen: ebay airway onditionglay ispay orkningway\n",
            "Epoch:  18 | Train loss: 0.389 | Val loss: 0.896 | Gen: ebay airway ondutioniongway isway orknangway\n",
            "Epoch:  19 | Train loss: 0.411 | Val loss: 0.935 | Gen: ebay airway onditiongangcay ispay orknhay\n",
            "Epoch:  20 | Train loss: 0.347 | Val loss: 0.757 | Gen: eway airway onditioniongway isway orkingway\n",
            "Epoch:  21 | Train loss: 0.327 | Val loss: 0.909 | Gen: ebay airway onditioningcay isway orkingpay\n",
            "Epoch:  22 | Train loss: 0.310 | Val loss: 0.807 | Gen: ebay airway onditioniongway isway orkingway\n",
            "Epoch:  23 | Train loss: 0.297 | Val loss: 0.686 | Gen: ebay airway onditiongay isway orkningway\n",
            "Epoch:  24 | Train loss: 0.286 | Val loss: 1.071 | Gen: ehay airay onditioniongay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.339 | Val loss: 1.137 | Gen: ebay airway onditionionccay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.382 | Val loss: 0.732 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.278 | Val loss: 0.689 | Gen: ebay airway onditioniongcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.235 | Val loss: 0.615 | Gen: ebay airway onditioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.221 | Val loss: 0.648 | Gen: ebray airway onditioniongcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.206 | Val loss: 0.604 | Gen: ebay airway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.241 | Val loss: 0.746 | Gen: ebay airway onditionicgcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.277 | Val loss: 1.066 | Gen: ehay airway onditiovvay isway orkingay\n",
            "Epoch:  33 | Train loss: 0.264 | Val loss: 0.775 | Gen: ehay airway onditioningway isway orkingway\n",
            "Epoch:  34 | Train loss: 0.207 | Val loss: 0.722 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.173 | Val loss: 1.086 | Gen: ehay airway onditioniongcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.163 | Val loss: 0.766 | Gen: ehay airway onditioniongcay isway orkingpay\n",
            "Epoch:  37 | Train loss: 0.171 | Val loss: 1.321 | Gen: ehay airway onditioonioniongday isway orkingay\n",
            "Epoch:  38 | Train loss: 0.238 | Val loss: 1.002 | Gen: ehay airay ontioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.293 | Val loss: 1.590 | Gen: ehsay airay onditioioioioodcay isway owungway\n",
            "Epoch:  40 | Train loss: 0.353 | Val loss: 0.767 | Gen: egay airway onditiongcay isway orknonway\n",
            "Epoch:  41 | Train loss: 0.212 | Val loss: 0.615 | Gen: eghay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.159 | Val loss: 0.592 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.146 | Val loss: 0.648 | Gen: ehay airway onditiongvay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.141 | Val loss: 0.769 | Gen: ehay airway onditiongcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.127 | Val loss: 0.560 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.108 | Val loss: 0.564 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.104 | Val loss: 0.754 | Gen: ehay airway onditioninccay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.155 | Val loss: 1.531 | Gen: ehay airway ondditioninininininc isway orkingway\n",
            "Epoch:  49 | Train loss: 0.224 | Val loss: 0.705 | Gen: ehay airway onditioningnay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.133 | Val loss: 0.595 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.124 | Val loss: 0.712 | Gen: ehay airway onditioningvay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.118 | Val loss: 0.721 | Gen: ehay airway onditioningncay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.095 | Val loss: 0.589 | Gen: ehay airway onditioningvay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.086 | Val loss: 0.597 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.079 | Val loss: 0.592 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.072 | Val loss: 0.612 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.069 | Val loss: 0.584 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.066 | Val loss: 0.887 | Gen: ehay airway onditiongcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.087 | Val loss: 0.782 | Gen: ehay airway onditionininccay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.188 | Val loss: 1.532 | Gen: ehay airway onditioninccay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.289 | Val loss: 1.058 | Gen: ethay airway onditioniningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.206 | Val loss: 0.992 | Gen: ehay airway onditionicccay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.203 | Val loss: 0.694 | Gen: ehay airway onditioniningday isway orkingway\n",
            "Epoch:  64 | Train loss: 0.154 | Val loss: 0.712 | Gen: ehay airway onditionincncay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.129 | Val loss: 0.634 | Gen: ehay airway onditioningvay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.089 | Val loss: 0.602 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.079 | Val loss: 0.557 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.065 | Val loss: 0.559 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.058 | Val loss: 0.559 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.054 | Val loss: 0.567 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.051 | Val loss: 0.562 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.048 | Val loss: 0.569 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.046 | Val loss: 0.569 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.044 | Val loss: 0.577 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.044 | Val loss: 0.581 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.041 | Val loss: 0.572 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.040 | Val loss: 0.595 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.040 | Val loss: 0.573 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.037 | Val loss: 0.584 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.034 | Val loss: 0.580 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.032 | Val loss: 0.595 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.031 | Val loss: 0.616 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.032 | Val loss: 0.770 | Gen: ebshay airway onditioiningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.112 | Val loss: 1.146 | Gen: ethay airway onditioningcay isway orkincay\n",
            "Epoch:  85 | Train loss: 0.170 | Val loss: 0.669 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.104 | Val loss: 0.675 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.060 | Val loss: 0.624 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.042 | Val loss: 0.593 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.035 | Val loss: 0.581 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.032 | Val loss: 0.580 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.029 | Val loss: 0.577 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.028 | Val loss: 0.579 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.026 | Val loss: 0.577 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.025 | Val loss: 0.576 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.024 | Val loss: 0.581 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.023 | Val loss: 0.579 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.022 | Val loss: 0.584 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.021 | Val loss: 0.583 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.020 | Val loss: 0.587 | Gen: ethay airway onditioningcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X8FaZZUWRpY9"
      },
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ]
    },
    {
      "metadata": {
        "id": "Ik5rx9qw9KCg",
        "outputId": "ed9efe46-e1dc-4dbf-dc6b-16590be4e9e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2376
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('probability', 'obabilitypray')\n",
            "('grievance', 'ievancegray')\n",
            "('advised', 'advisedway')\n",
            "('affront', 'affrontway')\n",
            "('laudable', 'audablelay')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type TransformerDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type CausalScaledDotAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type ScaledDotAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train loss: 2.018 | Val loss: 1.837 | Gen: etetetetetetetetetet awaway ontintiooontiooooooo isssssssssssssssssss ingay-inay\n",
            "Epoch:   1 | Train loss: 1.373 | Val loss: 1.402 | Gen: ethethethethethethet iwaway onnnngingingingdingd isssssssssssssssssss onghingway\n",
            "Epoch:   2 | Train loss: 0.998 | Val loss: 1.166 | Gen: etheay inway ongingingingindindin isway owlingway\n",
            "Epoch:   3 | Train loss: 0.760 | Val loss: 1.004 | Gen: ethay awaway ingsingcayngcay isay aynghinghway\n",
            "Epoch:   4 | Train loss: 0.603 | Val loss: 0.893 | Gen: ethethethethethethet iwrway ondingcayngcay isway owkrkingway\n",
            "Epoch:   5 | Train loss: 0.510 | Val loss: 0.900 | Gen: ethay airwairwairwairwairw ondingcongcay isway orkwayway\n",
            "Epoch:   6 | Train loss: 0.447 | Val loss: 0.720 | Gen: ethay awaway ondindcayiongcay isway owlingway\n",
            "Epoch:   7 | Train loss: 0.383 | Val loss: 0.614 | Gen: ethay airway onditingcaydioday isway orkingway\n",
            "Epoch:   8 | Train loss: 0.305 | Val loss: 0.543 | Gen: ethay airway ondindcaingcay isway orkingway\n",
            "Epoch:   9 | Train loss: 0.254 | Val loss: 0.505 | Gen: eththay airwaiaiaiaiaiaiaiai ondindcayniongcccccc isway orkingway\n",
            "Epoch:  10 | Train loss: 0.227 | Val loss: 0.457 | Gen: ethay airway onditiongcayday isway orkingway\n",
            "Epoch:  11 | Train loss: 0.246 | Val loss: 0.627 | Gen: eththay airwarararararararar ondionionionioy isway orkingway\n",
            "Epoch:  12 | Train loss: 0.227 | Val loss: 0.474 | Gen: ethay airway onditiongcagcccccccc isway orkingway\n",
            "Epoch:  13 | Train loss: 0.189 | Val loss: 0.414 | Gen: ethay airwayayayayayayayay onditioningcay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.161 | Val loss: 0.364 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.195 | Val loss: 0.597 | Gen: eththay airway onditiongcayiodcay isisay orkingway\n",
            "Epoch:  16 | Train loss: 0.215 | Val loss: 0.468 | Gen: ethay airway onditiongcagcay isisay orkingway\n",
            "Epoch:  17 | Train loss: 0.174 | Val loss: 0.404 | Gen: ethay airay onditiongcayay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.161 | Val loss: 0.373 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.148 | Val loss: 0.425 | Gen: ethay airway onditioningay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.133 | Val loss: 0.305 | Gen: ethay airway onditiongcagcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.129 | Val loss: 0.381 | Gen: ethay airway onnditiongcay isisway ortingway\n",
            "Epoch:  22 | Train loss: 0.122 | Val loss: 0.342 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.111 | Val loss: 0.339 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.111 | Val loss: 0.331 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.109 | Val loss: 0.302 | Gen: ethay airway onditionsgcay isway orkikincay\n",
            "Epoch:  26 | Train loss: 0.106 | Val loss: 0.453 | Gen: ethay airay onditioningcay isi orkingway\n",
            "Epoch:  27 | Train loss: 0.132 | Val loss: 0.385 | Gen: ethay arwayayayayayayayaya onditioniongcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.112 | Val loss: 0.361 | Gen: ethay airway ondionioningway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.132 | Val loss: 0.336 | Gen: ethay airway ondionioniongcay isisay orkingway\n",
            "Epoch:  30 | Train loss: 0.110 | Val loss: 0.297 | Gen: ethay airway onditioningacay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.090 | Val loss: 0.215 | Gen: ethay airway ondiondiongcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.078 | Val loss: 0.245 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.069 | Val loss: 0.266 | Gen: ethay airway onditioniongay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.074 | Val loss: 0.272 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.074 | Val loss: 0.408 | Gen: ethay airway onditioningcay isway orkikingway\n",
            "Epoch:  36 | Train loss: 0.100 | Val loss: 0.290 | Gen: ethay airay onditioningcay isiy orkingway\n",
            "Epoch:  37 | Train loss: 0.072 | Val loss: 0.285 | Gen: ethay airway ondititiongcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.072 | Val loss: 0.228 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.073 | Val loss: 0.375 | Gen: ethay airay onditionioncay isisway orkingway\n",
            "Epoch:  40 | Train loss: 0.091 | Val loss: 0.294 | Gen: ethay aiway onditioningcay isway orkikingway\n",
            "Epoch:  41 | Train loss: 0.082 | Val loss: 0.267 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.073 | Val loss: 0.328 | Gen: ethay airwayayayayayayayay onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.122 | Val loss: 0.720 | Gen: ethayayayayayayayaya ayeray onditioninininay isway orkingwayngwayngwayn\n",
            "Epoch:  44 | Train loss: 0.152 | Val loss: 0.263 | Gen: ethay airway onditioningcay isiay orkingway\n",
            "Epoch:  45 | Train loss: 0.074 | Val loss: 0.198 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.062 | Val loss: 0.179 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.058 | Val loss: 0.182 | Gen: ethay awway onditioniongay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.054 | Val loss: 0.162 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.049 | Val loss: 0.170 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.055 | Val loss: 0.213 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.052 | Val loss: 0.163 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.041 | Val loss: 0.249 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.059 | Val loss: 0.238 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.069 | Val loss: 0.329 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.081 | Val loss: 0.233 | Gen: ethay airway onditioninininay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.060 | Val loss: 0.220 | Gen: ethay airay onditioningcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.050 | Val loss: 0.214 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.052 | Val loss: 0.178 | Gen: ethay arway onditioningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.039 | Val loss: 0.171 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.037 | Val loss: 0.177 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.032 | Val loss: 0.153 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.034 | Val loss: 0.193 | Gen: ethay arway onditioningcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.033 | Val loss: 0.182 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.042 | Val loss: 0.233 | Gen: ethay arway onditioningcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.038 | Val loss: 0.205 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.047 | Val loss: 0.280 | Gen: ethay airay onditionininay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.093 | Val loss: 0.532 | Gen: ethay awaway onditioninggay isway orkinggway\n",
            "Epoch:  68 | Train loss: 0.199 | Val loss: 0.351 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.102 | Val loss: 0.196 | Gen: ethay airway ondionioningcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.055 | Val loss: 0.164 | Gen: ethay airay onditioningcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.046 | Val loss: 0.158 | Gen: ethay airway onditininingcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.035 | Val loss: 0.140 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.031 | Val loss: 0.142 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.027 | Val loss: 0.149 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.029 | Val loss: 0.122 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.028 | Val loss: 0.168 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.035 | Val loss: 0.154 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.028 | Val loss: 0.150 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.029 | Val loss: 0.141 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.026 | Val loss: 0.160 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.026 | Val loss: 0.151 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.023 | Val loss: 0.148 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.027 | Val loss: 0.155 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.025 | Val loss: 0.174 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.029 | Val loss: 0.170 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.042 | Val loss: 0.264 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.050 | Val loss: 0.252 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.073 | Val loss: 0.398 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.074 | Val loss: 0.285 | Gen: ethay airway onditininingay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.047 | Val loss: 0.226 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.035 | Val loss: 0.173 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.027 | Val loss: 0.155 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.024 | Val loss: 0.168 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.023 | Val loss: 0.149 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.025 | Val loss: 0.179 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.023 | Val loss: 0.163 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.028 | Val loss: 0.180 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.025 | Val loss: 0.146 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.024 | Val loss: 0.158 | Gen: ethay airway onditioningcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ULCMHm5ZF7vx",
        "outputId": "75598a8e-1d83-4cce-dfe4-ad9d7f7b51b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the quick brown fox jumps over the lazy dog'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-933eda9b2ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTEST_SENTENCE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'the quick brown fox jumps over the lazy dog'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"source:\\t\\t{} \\ntranslated:\\t{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transformer_encoder' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qbfZCByITOI6"
      },
      "cell_type": "markdown",
      "source": [
        "# Attention visualization"
      ]
    },
    {
      "metadata": {
        "id": "itCGMv3FdXsn"
      },
      "cell_type": "code",
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "#TEST_WORD_ATTN = 'czech'\n",
        "TEST_WORD_ATTN = 'greeneyed'\n",
        "#TEST_WORD_ATTN = 'xylophone'\n",
        "#TEST_WORD_ATTN = 'xylophone'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xBv4QQuBiU-V"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize RNN attention map"
      ]
    },
    {
      "metadata": {
        "id": "aXvqoQYONMTA",
        "outputId": "1ab5947c-91f8-4068-ab44-3c0aef62376e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "cell_type": "code",
      "source": [
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAGBCAYAAACKBaN2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eeneyedgray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "metadata": {
        "id": "xuOvxfA1NMz3"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize transformer attention maps from all the transformer layers"
      ]
    },
    {
      "metadata": {
        "id": "HSSB4wd8-M7g",
        "outputId": "c0af3364-adda-47ef-ba32-e7144e25755a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        }
      },
      "cell_type": "code",
      "source": [
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAF5CAYAAAARXWmZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIAlkgUAAGCJZAEAAFj6f4t12AOtfPZsAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAF9CAYAAACKzCuPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAIABJREFUeJzt3Xl0VFW69/FfZWwhUQMmzLSAIEKD\ndIKgHRSJKNCiARokrRAvsFiNIrQaGmMQomCAphkEgiy1ucBlEoTIoGkRuCooYWjmQVtFCKBAEuYw\nCEnt9w8u9RIhhwjJOVWV74dVa9W491OnDqmnnr33OS5jjBEAAEAxApwOAAAAeDeSBQAAYIlkAQAA\nWCJZAAAAlkgWAACAJZIFAABgiWQBPmX58uVOh4D/c+bMGcXFxTkdBgAbkCzAZxw8eFAff/yx02EA\nQLkT5HQAxSksLNTQoUN14MABFRQUaODAgXrggQfKXQwZGRnatGmTjh07pr1796pPnz7q1q2brTF4\nw3aQpOHDh2v79u1KT0/XCy+8YGvf+fn5SkpK0tmzZ3X+/HkNHTpUTZs2tTUGb/gc8vPzNWDAAP38\n88+KiYmxte/LTp8+rYEDB+r8+fNq3bq1FixYoP/93/+1PY6MjAytXr1aOTk5mjBhgqpUqWJb396w\nP3br1k3jxo1T7dq1dfjwYT3//PPKyMiwNQbYx2srC8uWLVNkZKRmzZqlKVOmaOTIkeUyBkn69ttv\nlZ6erilTpmj27Nm29+8t26FPnz5q0aKF7YmCJOXm5qpbt26aNWuWXn75Zb333nu2x+ANn8OSJUtU\nv359zZ07V/fcc4/t/UvS4sWLVa9ePc2bN0/h4eGOxHDZoUOHNGfOHFsTBck79sf4+HhlZmZKklat\nWqXHH3/c9hh8kTGmVC5289rKwpYtW7Rp0yZt3rxZkvTzzz/rwoULCgkJKVcxSFKzZs0UGBioqlWr\n6vTp07b2LXnPdnDSHXfcobffflvTpk3ThQsXVKFCBdtj8IbPYc+ePbrvvvskSS1atLCt31/GcLnv\nRx55RNOmTXMkDklq0qSJXC6X7f16w/74+OOPq0+fPurXr58+//xzvfnmm7bHAPt4bbIQHBysfv36\nqWPHjuU6BkkKCnL2Y/KW7eCkmTNnqkqVKvrHP/6hHTt2aMyYMbbH4A2fgzFGAQGXCpJut9vxGJz4\nor5ScHCwI/16w/4YERGhqlWravv27XK73bZXV3yVu5SqAoE27/teOwxx7733atWqVZKko0ePavz4\n8eUyBm/gLdshICBABQUFjvR9/Phx1a5dW5K0cuVKXbx40fYYvOFzqFOnjnbu3ClJWr9+ve39S1Lt\n2rU9MaxevdqRGJzmDfujdGkoYvjw4Wrfvr0j/fsiXx2G8NpkoUOHDqpQoYISEhLUr18/RyZTeUMM\n3sBbtkO9evW0e/duR8bq4+PjNX36dPXu3VtNmzZVbm6uFi1aZGsM3vA5dOrUSVu3btWzzz6rvXv3\n2t6/JHXu3Fn//ve/1bNnT+Xl5XmqDOWJN+yPktSmTRvt379f7dq1s71v2MvFKaoB+JIff/xRP/zw\ngx588EFt2bJFkydP1n//9387HVa5tG7dOn344Yf6+9//7nQoPuNiYelUR4MD7R2e9to5CwBwLeHh\n4ZoxY4amTJkiSRoyZIjDEZVPkyZN0pdffqnJkyc7HYpPcfvoz3MqCwAA2OTnUpp3FWrzxHcqCwAA\n2MRXf5+TLAAAYJPSWjppN5IFAABs4quVhfK35ggAAPwqVBYAALCJr1YWSBYAALAJcxaK4fSx271F\ncHCoo/0XltKBQG6G213odAjwIrfdFul0CDp5MtfR/l0u50eCjXHmHB9Xuv/+eKdDUFbWYqdD8GpU\nFgAAsAnDEAAAwJIRyQIAALDgq4d7dn7ADAAAeDUqCwAA2IQ5CwAAwJKvLp1kGAIAAFiisgAAgE0Y\nhgAAAJZIFgAAgCXmLAAAAL9EZQEAAJswDAEAACz59eGeH3nkkavuCwwMVK1atfTyyy+rcePGpR4Y\nAADwDiVKFp566imFh4d7kobVq1fr2LFjatmypd58803NmzevTIMEAMAf+PW5IVavXq2nn35aVapU\nUZUqVdStWzd99dVXatasWVnHBwCA3zDGlMrFbiWqLISGhmrkyJGKjo5WQECAdu7cqYsXL+qrr75S\nhQoVyjpGAAD8gq9OcHSZEkSen5+vxYsXa8+ePTLGqHbt2urcubPOnTun8PBwhYeHF9+By1WqAfuq\n4OBQR/svLCxwtH9JcrsLnQ4BXuS22yKdDkEnT+Y62r/L5fzqdWPcToeg+++PdzoEZWUttqWf7Ly8\nUmnnt3fcUSrtlFSJKgthYWHq0aPHVfdHRESUekAAAPgrXz0oE0snAQCwia8OQzhfAwMAAF6NygIA\nADZhGAIAAFjy1WEIkgUAAGziq4d7Zs4CAACwRGUBAACb+OrhnkkWAACwia/OWWAYAgAAWKKyAACA\nTXy1skCyAACATTjOAgAAsERlAZYuXvzZ4Qg4+6ckBQWFOB2CCgouOB2CV7hw4bzTIaig0Nkzod5+\nm71nDryW/PzjToeg3kOfdzoEXAfJAgAANqGyAAAALPnqnAWWTgIAAEtUFgAAsImvnhuCZAEAAJtw\nuGcAAGDJVyc4MmcBAABYorIAAIBNfLWyQLIAAIBNWDoJAAD8EpUFAABswjAEAACwRLIAAAAs+fWc\nhZycnLKOAwAAeKkSJQsvv/xyWccBAIDfM6X0z24lGoaIjIxUQkKCmjRpouDgYM/9gwcPLrPAAADw\nN359uOeHHnqorOMAAABeqkTJQufOncs6DgAA/B6rIQAAgCWSBQAAYMnOpZMjR47Utm3b5HK5lJKS\noqZNm3oemzNnjpYuXaqAgAD97ne/05AhQyzb4nDPAAD4mQ0bNig7O1vz589XWlqa0tLSPI/l5+dr\n2rRpmjNnjubNm6c9e/Zo69atlu2RLAAAYBNjTKlcricrK0tt27aVJNWrV08nT55Ufn6+JCk4OFjB\nwcE6e/asCgoKdO7cOd12222W7TEMAQCATeyas5CXl6fGjRt7bleqVEm5ubkKCwtTaGio+vfvr7Zt\n2yo0NFSPP/646tSpY9kelQUAAPzclUlKfn6+3nnnHX3yySdatWqVtm3bpm+++cby9SQLAADYxG1M\nqVyuJyoqSnl5eZ7bOTk5ioyMlCTt2bNHtWrVUqVKlRQSEqLmzZtr586dlu2RLAAAYBO7DvccGxur\n5cuXS5J27dqlqKgohYWFSZJq1KihPXv26Pz585KknTt36s4777RsjzkLAADYxK6Vk9HR0WrcuLES\nEhLkcrmUmpqqjIwMhYeH69FHH1WfPn2UmJiowMBA/f73v1fz5s0t2yNZAADADw0aNKjI7YYNG3qu\nJyQkKCEhocRtkSwAAGATOw/KVJpIFgAAsImvHu6ZCY4AAMASlQUAAGzCMEQxvKHkEhwc6nQIKii4\n4HAEzn8O3uDFoeOdDkFhEWFOh6DXB/6X0yHo3Ll8p0NQcFCwo/3vzTniaP+SlPin/k6HoL907OB0\nCOrrLrSlH2/4TrwRVBYAALCJryYLzFkAAACWqCwAAGAT5iwAAABLJTlUszdiGAIAAFiisgAAgE18\ndBSCZAEAALswZwEAAFhi6SQAAPBLVBYAALAJwxAAAMASwxAAAMAvUVkAAMAmvlpZ+FXJQkFBgYKC\nyC8AALghPposlGgYYt26dXryySfVsWNHSdKECRO0Zs2aMg0MAAB/Y9ymVC52K1GyMHnyZM2cOVOR\nkZGSpMTERKWnp5dpYAAAwDuUaEwhKChIERERcrlckqTKlSt7rgMAgJLx0VGIkiULNWvW1MSJE3X8\n+HFlZmZq5cqVql+/flnHBgCAX/HrCY4jRozQsmXLFBMToy1btiguLk4dOnQo69gAAIAXKFGyEBAQ\noPj4eMXHx5d1PAAA+C2/riwAAICb56vJAkdwBAAAlqgsAABgEyeOkVAaSBYAALCJrw5DkCwAAGAT\nX00WmLMAAAAsUVkAAMAuPlpZIFkAAMAmPporMAwBAACsUVkAAMAmLJ0EAACWfHU1BMkCAAA2IVko\nhsvlKusurmvlzp1Oh6A3nnvD0f7XrPnA0f4lyeVyforM2NQBTofgFf8nvIPzfzSd/sN9Z2Sko/1L\nUkBAoNMhyBi30yHgOqgsAABgE6cT1BtFsgAAgE18NVlwvi4MAAC8GpUFAADswtJJAABgxVeHIUgW\nAACwiY/mCsxZAAAA1qgsAABgE4YhAACAJV9NFhiGAAAAlqgsAABgE846CQAALPnqMATJAgAANvHV\nZOFXzVkoKCgoqzgAAICXKlGysG7dOj355JPq2LGjJGnChAlas2ZNmQYGAIC/McaUysVuJUoWJk+e\nrJkzZyry/869npiYqPT09DINDAAAv2NM6VxsVqJkISgoSBEREXK5XJKkypUre64DAAD/VqIJjjVr\n1tTEiRN1/PhxZWZmauXKlapfv35ZxwYAgF8xbqcjuDElShZGjBihZcuWKSYmRlu2bFFcXJw6dOhQ\n1rEBAOBXfHU1RImShYCAAMXHxys+Pr6s4wEAwG/5arLA4Z4BAIAlDsoEAIBNfLWyQLIAAIBNfDVZ\nYBgCAABYorIAAIBNOOskAACwZuMwxMiRI7Vt2za5XC6lpKSoadOmnscOHTqkl19+WRcvXlSjRo00\nfPhwy7YYhgAAwCZ2nRtiw4YNys7O1vz585WWlqa0tLQij48ePVq9e/fWwoULFRgYqJ9++smyPZIF\nAAD8TFZWltq2bStJqlevnk6ePKn8/HxJktvt1qZNmxQXFydJSk1NVfXq1S3bI1kAAMAmdp1HKi8v\nTxEREZ7blSpVUm5uriTp2LFjqlixokaNGqU///nPGjdu3HXbI1kAAMAmTp2i+srXGGN05MgRJSYm\navbs2dq9e7c+//xzy9eTLAAA4GeioqKUl5fnuZ2Tk6PIyEhJUkREhKpXr67atWsrMDBQDzzwgL77\n7jvL9kgWAACwiXGbUrlcT2xsrJYvXy5J2rVrl6KiohQWFiZJCgoKUq1atbRv3z7P43Xq1LFsr1ws\nnez5qPMnwGrb4WlH+//qq0BH+5ekWrUaOh2CsrN3OR2Czx7BDf7J7S50OoRyxa7//9HR0WrcuLES\nEhLkcrmUmpqqjIwMhYeH69FHH1VKSoqSk5NljFGDBg08kx2LUy6SBQAAvIGdPxYGDRpU5HbDhv//\nB9tvf/tbzZs3r8RtMQwBAAAsUVkAAMAmvjoMSbIAAIBNfDVZYBgCAABYorIAAIBdOOskAACw4qOj\nECQLAADYhTkLAADAL1FZAADAJr5aWSBZAADAJiU5r4M3YhgCAABYorIAAIBN/HoYYtGiRZo1a5by\n8/NljJExRi6XS6tWrSrr+AAA8Bt+nSxMmzZN6enpqlq1alnHAwCA//LnZOHOO+9U3bp1yzoWAADg\nhUqULFSqVEndu3dXs2bNFBgY6Ll/8ODBZRYYAAD+xq+HIWJiYhQTE1PWsQAA4NeM2+kIbkyJkoXO\nnTuXdRwAAMBLsXQSAACb+PUwBAAAuHkkCwAAwJKvJgsc7hkAAFiisgAAgE18tbJAsgAAgE046yQA\nAPBLVBYAALAJwxAAAMAayQIAALDio7kCcxYAAIA1KgsAANiEOQte7OjRH50OQR/MneBo/0dPn3K0\nf0n6bdXaToegZ54d4nQImjMzzekQADiEpZMAAMAvlYvKAgAA3oBhCAAAYIlkAQAAWPLVZIE5CwAA\nwBKVBQAA7OKjlQWSBQAAbMLSSQAA4JeoLAAAYBMfHYUgWQAAwC6shgAAAH6JygIAADbx1coCyQIA\nADYpF8lCQUGBgoLILwAAuBF+vXRy3bp1evLJJ9WxY0dJ0oQJE7RmzZoyDQwAAHiHEiULkydP1syZ\nMxUZGSlJSkxMVHp6epkGBgCAvzHGlMrFbiUaUwgKClJERIRcLpckqXLlyp7rAACghPx5zkLNmjU1\nceJEHT9+XJmZmVq5cqXq169f1rEBAAAvUKJkYcSIEVq2bJliYmK0ZcsWxcXFqUOHDmUdGwAAfsWv\nV0MEBAQoPj5e8fHxZR0PAAB+y0dzBY6zAACAXfx66SQAACi/qCwAAGATv56zAAAAbp6vJgsMQwAA\nAEtUFgAAsImvVhZIFgAAsAnJAgAAsMTSSQAA4JeoLAAAYBeGIQAAgBUfzRUYhgAAANbKRWXBG2af\n3tPwfkf7j6hY0dH+JemPf/yL0yFo3qzRTofgFSIiqjodgo4fP+J0CJKc/9uA8sXO76ORI0dq27Zt\ncrlcSklJUdOmTa96zrhx47R161bNmjXLsq1ykSwAAOAN7EoWNmzYoOzsbM2fP1979uxRSkqK5s+f\nX+Q533//vTZu3Kjg4ODrtscwBAAANjFuUyqX68nKylLbtm0lSfXq1dPJkyeVn59f5DmjR4/WSy+9\nVKK4SRYAAPAzeXl5ioiI8NyuVKmScnNzPbczMjLUokUL1ahRo0TtkSwAAGATY0ypXG6k38tOnDih\njIwM9erVq8SvZ84CAAA2sWvOQlRUlPLy8jy3c3JyFBkZKUlat26djh07pmeeeUYXLlzQ/v37NXLk\nSKWkpBTbHpUFAAD8TGxsrJYvXy5J2rVrl6KiohQWFiZJat++vTIzM7VgwQKlp6ercePGlomCRGUB\nAADb2FVZiI6OVuPGjZWQkCCXy6XU1FRlZGQoPDxcjz766K9uj2QBAAC72HichUGDBhW53bBhw6ue\nU7NmzeseY0EiWQAAwDbG7XQEN4Y5CwAAwBKVBQAAbOINpx+4ESQLAADYxFeTBYYhAACApesmC126\ndNH06dOVk5NjRzwAAPgtp47geLOumyxMnTpVoaGheu2119S3b18tXLjwqpNRAACA6/PbZKFKlSp6\n+umn9e6772rgwIGaP3++HnnkEb366qtUGwAA+BXsOutkabvuBMcDBw4oMzNTK1asUNWqVdW3b1+1\nadNGmzZt0sCBA/X+++/bEScAAHDIdZOFpKQkxcfH65///Kduv/12z/3333+/YmNjyzQ4AAD8io+u\nhrhusrBgwYJiHxswYECpBgMAgD8z8s1kgaWTAADAEgdlAgDAJr56UCaSBQAAbGJ89ExSJAsAANjE\nVysLzFkAAACWqCwAAGATX60skCwAAGATX00WGIYAAACWqCwAAGATVkMAAABrPjoMQbIAAIBNONwz\nAADwS1QWAACwia+uhigXycLFiz87HYK2bF3pcAQuh/uXMjPfcToEr+ByOV/QO378iNMhqEKFcKdD\n0NmzpxyOwPn/l6Ehv3E6BAUElouvIkm+myw4/1cLAAB4tfKTzgEA4DCWTgIAAEu+OgxBsgAAgE18\nNVlgzgIAALBEZQEAAJv4amWBZAEAALv4aLLAMAQAALBEZQEAAJsYsXQSAABYYM4CAACw5KvJAnMW\nAACAJSoLAADYxFcrC78qWSgoKFBQEPkFAAA3wlfPDVGiYYh169bpySefVMeOHSVJEyZM0Jo1a8o0\nMAAA4B1KlCxMnjxZM2fOVGRkpCQpMTFR6enpZRoYAAD+xhhTKhe7lWhMISgoSBEREXK5XJKkypUr\ne64DAICS8es5CzVr1tTEiRN1/PhxZWZmauXKlapfv35ZxwYAgH/x52RhxIgRWrZsmWJiYrRlyxbF\nxcWpQ4cOZR0bAADwAiVKFgICAhQfH6/4+PiyjgcAAL9l5MeVBQAAcPP8eukkAAAov6gsAABgE79e\nDQEAAG4eyQIAALDkq8kCcxYAAIAlKgsAANjEV1dDkCwAAGAThiEAAIBforIAAIBdfLSyQLIAAIBN\nfPVwzwxDAAAAS1QWbOJyOZuXecMM3D7Pj3A6BM145w2nQ5Db7fxnsWXfXqdD0MmzZ50OQQ83auRw\nBM7/ynR7wd+Gn8+ddjoE2/jqBEeSBQAAbOINP9xuBMkCAAA28dXKAnMWAACAJSoLAADYxFcrCyQL\nAADYhGQBAAB4jZEjR2rbtm1yuVxKSUlR06ZNPY+tW7dO48ePV0BAgOrUqaO0tDQFBBQ/M4E5CwAA\n2MQYUyqX69mwYYOys7M1f/58paWlKS0trcjjw4YN06RJk/T+++/rzJkzWrNmjWV7VBYAALCLTUsn\ns7Ky1LZtW0lSvXr1dPLkSeXn5yssLEySlJGR4bleqVIlHT9+3LI9KgsAANjElNK/68nLy1NERITn\ndqVKlZSbm+u5fTlRyMnJ0VdffaXWrVtbtkeyAACAn7vW0MXRo0fVr18/paamFkksroVhCAAAbGLX\naoioqCjl5eV5bufk5CgyMtJzOz8/X3379tWLL76oVq1aXbc9KgsAANjErgmOsbGxWr58uSRp165d\nioqK8gw9SNLo0aP17LPP6qGHHipR3FQWAADwM9HR0WrcuLESEhLkcrmUmpqqjIwMhYeHq1WrVlq8\neLGys7O1cOFCSVLHjh3VvXv3YtsjWQAAwCZ2nkhq0KBBRW43bNjQc33nzp2/qi3LZCEuLk4ul+ua\nj7lcLq1cufJXdQYAQHnml0dw/Oijj2SM0TvvvKOGDRuqZcuWcrvdWrdunbKzs+2KEQAAv+CryYLl\nBMcKFSqoYsWK2rx5s/74xz+qcuXKioyM1BNPPKFNmzbZFSMAAHBQieYshISEaPTo0fr973+vgIAA\n7dixQ4WFhWUdGwAAfsVXKwslShYmTZqkpUuXasOGDTLGqE6dOpoyZUpZxwYAgH/x52QhLCxMTz/9\ndFnHAgAAvBBLJwEAsImRfUsnSxPJAgAANvHrOQsAAODm+WqywLkhAACAJSoLAADYxFcrCyQLAADY\nxM5zQ5QmhiEAAIAlKgsAANiEYQgAAGCJZAEAAFjz0WSBOQsAAMASlQUAAGxi5JuVBZcp4wEUl8tV\nls0DPifh6WSnQ9DSxW87HYLOnj3ldAiOCwgIdDoEjZ+zyOkQ9OKfOzkdgm1zCX7720al0k529u5S\naaekGIYAAACWGIYAAMAmrIYAAACWSBYAAIAlX00WmLMAAAAsUVkAAMAmvlpZIFkAAMAmnHUSAAD4\nJSoLAADYhWEIAABgxVcP90yyAACATXx1giNzFgAAgCUqCwAA2MRXV0OQLAAAYBO/HIYoKCjQZ599\n5rm9du1apaSkaOrUqTp//nyZBwcAAJxnmSykpqbqiy++kCTt379fL730klq0aCGXy6U33njDlgAB\nAPAXxphSudjNchjiu+++04IFCyRJy5YtU/v27dWpUydJUs+ePcs+OgAA/IhfDkOEhoZ6rq9du1at\nW7cu84AAAPBXfllZuOWWW7R8+XKdOnVK+/btU2xsrCRpz549tgQHAACcZ5ksjBgxQm+99ZZOnz6t\nt99+W6Ghofr555/13HPPady4cXbFCACAf/DHpZNVqlTRqFGjitwXGhqq5cuXy+VylWlgAAD4G789\n3POiRYs0Y8YMnThxQi6XS3fccYd69eqlJ554wo74AACAwyyThXnz5ikrK0vvvvuuqlWrJkn68ccf\n9fe//11Hjx7Vf/3Xf9kRIwAAfsEvV0N88MEHGj9+vCdRkKQaNWpo3LhxWrp0aZkHBwCAP/HL1RAh\nISEKCrr6KcHBwQoJCSmzoAAA8Ee+em6I65518vDhw1fdd+DAgTIJBgAAeB/LysKAAQPUq1cvJSYm\nqlGjRiosLNSOHTs0d+5c/eMf/7ArRgAA/IKvzlmwTBaaNGmiadOmad68efryyy8VEBCgunXrasaM\nGcrLy7MrRgAA/IKvJguWwxAvvPCCqlevrqSkJE2ZMkURERF66aWXVK1aNSoLAACUE5aVhV9mQPv2\n7Sv2MQAAYM1Xvzstk4VfHqXxyjfJERwBAPiV/DFZ+CUSBAAAbpyRby6ddBmLmkh0dLTq1q0r6VJV\nYe/evapbt66MMdq3b582bdpkW6AAAPi6ihVvLZV2zpw5VSrtlJRlsvDjjz9avrhGjRqlHhAAAP6q\nQoXwUmnn7NnTpdJOSVkmCwAAoPTccktYqbRz7lx+qbRTUtc9giMAACjfSBb8TE5Ojho1aqR33323\nyP2bN2/2HKb7+++/165du264jyVLlkiSvv76a40YMeLGg71Jq1ev1tSpUy2fk5ycrA8++OCq+8+d\nO6dPP/20xH1duf1K4siRI8rKypIkTZ48WRMmTCjxa8uLy/uRnUqyz1ypZ8+eWrt2bRlG9P/95z//\nUY8ePdSjRw899dRTN/V/FN7LV08kRbLgZxYvXqx69eopIyOjyP0ZGRmeL7sVK1Zo9+7dN9T+kSNH\n9P7770uS7rnnHg0dOvTmAr4JDz30kJ577rkbeu3u3bt/VbJw5fYrifXr12vdunU3Elq5cOV+ZKeb\n2WfKWkpKivr376/Zs2frL3/5i0aPHu10SCgDvpos/Kqlk/B+ixYt0uuvv67k5GRt3rxZ0dHRWrFi\nhT755BNt375dHTp00OzZsxUWFqbf/OY3euihh5Samqpjx44pPz9fvXr10hNPPKHJkyfrxIkTOnz4\nsLKzs9WyZUsNHTpUSUlJ+vbbbzV48GD96U9/0ltvvaV58+Zp7969Sk1NlTFGBQUFSkpKUvPmzZWc\nnKyoqCh9++232rt3r7p27aq+fft64j1w4IAGDhyoDz/8UMYYxcbG6m9/+5s6d+6sjz/+WJs2bVJy\ncrKGDx+u7OxsnTlzRh07dlTv3r2VkZGhtWvXauzYsfriiy80btw43XbbbXrwwQc1e/ZsrV69WtKl\nX2z9+vXTvn371KVLFyUmJmrIkCE6deqUxowZo06dOmnYsGEKDg7W+fPn1b9/fz388MOeGK/cfq++\n+qqqVq16zfd65Xt66623ZIzR7bffLunSl+PAgQP1ww8/qEWLFho2bJgkafz48dq8ebPOnz+v++67\nT4MHDy6yRPnIkSMaNGiQJOn8+fPq3r27unbtarm9Y2Ji1K1bN0nS3XffrV27dmnq1Kk6ePCgfvrp\nJ73yyisKCwvT0KFD5Xa7FRoaqlGjRqlKlSqaNWuW/vWvf6mwsFB169ZVamqqfvOb33jiOXPmjJKS\nknTq1CkVFBSoTZs2eu6553Ty5Mkb3o/GjBlzzX7z8vL03HPPqVWrVtq+fbvOnDmjd955R1WqVNFn\nn32m9PR0hYaG6s4779Tw4cNEe+j3AAALF0lEQVTldruvuZ9c6cp9Ji4uTomJiVq9erUOHjyoN954\nQw888MA1/1+53W6lpqbqhx9+0IULF3TvvffqtddeU1JSkmJjY9WlSxdJUmpqqho0aKCOHTsWuz2u\n/Bx+97vfefqYMWOGwsIujWdXrlxZJ06cKMl/efgYXz3rpAz8xoYNG0xcXJxxu91m/PjxZsiQIZ7H\nevToYb766itjjDGvvPKKWbBggTHGmNdff90sXLjQGGPMmTNnTNu2bc3Ro0fNpEmTTEJCgikoKDDn\nzp0zzZo1MydOnDDr1q0zCQkJxhhT5Hrv3r1NZmamMcaYb775xsTFxXn6evHFF40xxhw8eNBER0df\nFfdjjz1mTp8+bb755hvTu3dvk5ycbIwxZujQoWbVqlXmvffeMxMnTjTGGFNQUGC6dOlivv76a7No\n0SKTlJRk3G63ad26tfn666+NMcaMHTvWPPjgg1f1f+jQIdOsWTNjjPG81hhjRowYYd555x1jjDF5\neXnmww8/vCrGK7dfce/1SpMmTTLjx4/3XE9ISDAXL14058+fN82aNTPHjh0zmZmZZvDgwZ7XPP/8\n82bVqlVF2pk+fboZNmyYMcaY8+fPm1mzZl13e1/+bI0xpkGDBubixYtm0qRJ5umnnzZut9sYY0xi\nYqL57LPPjDHGfPTRR2b69Olm27ZtpmfPnp7npKWlmf/5n/8pEs+nn35q+vTpY4wxprCw0MyYMcMU\nFhbe1H5UXL8HDhww99xzj/n222+NMcYkJyeb6dOnm7Nnz5o//OEP5ujRo8YYY8aMGWPWr19f7H5y\npSs/9zZt2pi5c+caY4zJyMgw/fr1u+pzvPy5Hzt2zLPtjTGmXbt25j//+Y/ZsGGD6dGjh6fPNm3a\nmFOnTllujys/h2txu93m+eefN9OnTy/2OfBdwcGhpXKxG5UFP7Jw4UJ17txZLpdLXbp0UZcuXTRk\nyBDdcsstxb5m/fr12rFjhxYvXixJCgoK0sGDByVJMTExCgwMVGBgoCIiInTy5Mli29m2bZtnXP7u\nu+9Wfn6+jh07Jklq0aKFpEtLbfPz81VYWKjAwEDPa++//35t2rRJ2dnZ6tSpk+bMmSPp0jyBV155\nRfPmzdPhw4e1ceNGSdKFCxe0f/9+z+uPHz+us2fPqmHDhpKkdu3aFRkPv9x/1apVdfbsWRUWFhaJ\nvV27dkpOTtZPP/2kNm3aKD4+vtj3afVeK1WqVOxrYmJiFBQUpKCgIEVEROj06dNav369tm7dqp49\ne0qSTp8+7dn2lz344IOaO3eukpOT1bp1a3Xv3v2627s49957r6dqsX37ds92efzxxyVJ7733nvbv\n36/ExERJ0tmzZxUUVPRPRHR0tCZNmqS//vWvat26tbp166aAgICb2o/Wr19fbL8RERGqX7++JKl6\n9eo6ceKEvv/+e1WtWtWzvf/2t7954r/WfnJ5v7iWy9ugevXqlvv3rbfeqkOHDql79+4KCQlRbm6u\njh8/rpYtW+rYsWM6cOCADh48qJiYGIWHh1tujys/h1+6ePGikpOTdeutt+rZZ58tNh74LuOjCxBJ\nFvxEfn6+Pv30U1WrVk0rVqyQdKl0unz5cnXq1KnY14WEhCg1NVVNmjQpcv8XX3xR5Atdst7Jr/XH\n7/J9v/zC+WU7rVq10saNG7V3714NGzZMK1as0LZt2xQREaGKFSsqJCRE/fv3V/v27Yu87vK8DGNM\nkf5/Gff1+r/vvvv00UcfKSsrSxkZGVq6dKnGjRt3Q++1ONfaliEhIXrqqafUp0+fYl9Xr149ffzx\nx9q4caM++eQTzZw5U++//36xMVx5/4ULF4o8HhwcXOS22120HBoSEqK4uDjPEMm1VK5cWUuWLNGW\nLVu0atUq/elPf9KHH354U/tRcf0ePHjwmq91uVzX3BeL20+sXLlvWO3fH3/8sXbs2KE5c+YoKCjI\nM+wgSd26ddPSpUt15MgRz/CP1fb45edwWWFhoQYMGKC77rpLSUlJHDHXX/lossAERz/x0Ucf6b77\n7lNmZqaWLFmiJUuWaPjw4Z4vVJfLpYsXL151PSYmRv/6178kXRoTf/3111VQUFBsPwEBAdd8/N57\n79WXX34p6dLkwdtvv10RERElir1ly5bavHmzcnNzVaVKFTVv3lxTp05Vq1atrorR7XZr1KhRRcZz\nIyIiFBAQoB9++EGSSjRx8cr3MWvWLB0+fFhxcXFKS0vTtm3brnr+ldusJO/V5XJZbsfL72vFihWe\n56Wnpxc5WZskLVu2TDt27NAf/vAHpaam6tChQyooKCg2hooVK+rQoUOSpKysrGK/cKKjo7VmzRpJ\nUmZmpsaPH6/o6GitXr1aZ86ckSTNmTNHW7ZsKfK6L7/8Up9//rliYmI0ePBgVahQQUePHr2p/agk\n/V6pbt26OnLkiA4fPixJGjVqlFauXHnd/eRmHD16VHXq1FFQUJB27typ/fv3e5KxTp06adWqVfrm\nm288lYpfuz0k6e2331adOnU0aNAgEgV4HSoLfmLhwoXq379/kfvatWun0aNH6+DBg4qNjVVqaqpS\nUlJ0//33a8yYMTLG6IUXXtBrr72mP//5z7pw4YK6d+9+1S/xK9111106evSoevXqpX79+nnuHzp0\nqFJTUzVv3jwVFBRozJgxJY791ltvldvtVoMGDSRdKg2PHDlSL7zwgiTpmWee0Xfffafu3bursLBQ\nDz/8sGfioHTpi+fyTPLq1aurefPmlu9Bkpo0aaKxY8fq1VdfVceOHZWUlKSKFSvK7XYrKSnpqudf\nuf1K8l6bN2+ul156ScHBwVf9Or7sscce09atW5WQkKDAwEA1atRItWrVKvKcu+66S6mpqQoJCZEx\nRn379lVQUFCxMXTt2lV//etftXHjRrVq1Urh4dc+WtzQoUM1dOhQzZ07V0FBQRo5cqSqVaumZ555\nRj179lRoaKiioqKK/IKWpDp16ig5OVn//Oc/FRgYqFatWqlGjRo3tR9Nnz79mv0ePXr0mq+tUKGC\n0tLSNGDAAIWEhKhmzZp6+OGHVVhYaLmf3Iz27durX79+6tGjh6Kjo9W7d2+9+eabWrBggW6//XbV\nqlVLjRs39jz/124PSZo2bZoaNGjgGZaSLk16LG7/gW8y8s3KAkdwhF9YuXKl7r77btWqVUuffvqp\n5s+fr2nTpjkdFsqBU6dOKSEhQXPmzClxNQ3lV2klf7+ce3UtI0eO1LZt2+RyuZSSkqKmTZt6Hlu7\ndq3Gjx+vwMBAPfTQQ1f92PwlKgvwC263WwMGDFBYWJgKCwv1+uuvOx0SyoGFCxdq5syZevHFF0kU\nUCJ2/T7fsGGDsrOzNX/+fO3Zs0cpKSmaP3++5/E333xT06ZNU5UqVdSjRw+1a9dOd911V7HtkSzA\nLzz22GN67LHHnA4D5UzXrl3VtWtXp8MArpKVlaW2bdtKujRR+uTJk8rPz1dYWJgOHDig2267TdWq\nVZMktW7dWllZWZbJAhMcAQCwibHpCI55eXlFql2VKlVSbm6uJCk3N7fIUu8rHysOlQUAAGzi1DTB\nm+2XygIAAH4mKipKeXl5nts5OTmKjIy85mNHjhxRVFSUZXskCwAA+JnY2FgtX75ckrRr1y5FRUV5\nzj1Ss2ZN5efn6+DBgyooKNBnn32m2NhYy/ZYOgkAgB8aO3as/v3vf8vlcik1NVW7d+9WeHi4Hn30\nUW3cuFFjx46VdGmCuNWRZCWSBQAAcB0MQwAAAEskCwAAwBLJAgAAsESyAAAALJEsAAAASyQLAADA\nEskCAACwRLIAAAAs/T8GKXy/OmDM2gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAF/CAYAAADHBIqEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLJEsAAAASyQLAADA0v8HQZlceIPe/FEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eenetaedgray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "metadata": {
        "id": "K2T45U4AbtU8"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}